{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WIP-Spring2019-SQuAD-BERT-TransferLearning-introOnly.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daveDoesData/IS7033/blob/master/WIP_Spring2019_SQuAD_BERT_TransferLearning_introOnly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "RHgPZ3mxHEqa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![illustration by 2mz https://www.teepublic.com/user/2mz](https://res.cloudinary.com/teepublic/image/private/s--VdF8cz3v--/t_Preview/b_rgb:191919,c_limit,f_jpg,h_630,q_90,w_630/v1446149825/production/designs/12295_0.jpg)\n",
        "\n",
        "illustration by 2mz https://www.teepublic.com/user/2mz\n",
        "\n",
        "# Intro\n",
        "2018 brought forth amazing capabilties in the realm of NLP transfer learning, and we learned how determined researchers can name a model after a Sesame Street character... no matter how much of a strech the acronym may be.  \n",
        "\n",
        "BERT, or Bidirectional Encoder Representations from Transformers, builds upon Transformers, GPT, and ELMo. BERT was designed for multiple language modeling task. BERT is unsupervised and was trained onlt on raw text without any designated labels. Generally language models are trained left and right, but BERT highly bi-directional. This unsupervised learning and bi-directinallity allow BERT to focus on the relationships between words in sentences. BERT's understanding of language in context makes it a powerful model to leverage in NLP transfer learning.  \n",
        "\n",
        "To understand the inner workings of BERT, we must first visit another Google creation the Transformer. \n",
        "\n",
        "\n",
        "## Transformers\n",
        "\n",
        "\n",
        "*   Transformers do not have any recurrent structures and instead they rely purely on attention.\n",
        "*   Positional encoding included with roken embedding to signify token's place in sequence\n",
        "*   The absense of a recurrent structure allows for better distributed processing \n",
        "*   Consist of encoder and decoders, but BERT only uses encoders\n",
        "\n",
        "High Level Explanation of Self-Attention:\n",
        "\"Say the following sentence is an input sentence we want to translate:\n",
        "\n",
        "”The animal didn't cross the street because it was too tired”\n",
        "\n",
        "What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\n",
        "\n",
        "When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\n",
        "\n",
        "As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
        "\n",
        "If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\"\n",
        "http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "Visual Examples: \n",
        "\n",
        "![https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html](https://3.bp.blogspot.com/-aZ3zvPiCoXM/WaiKQO7KRnI/AAAAAAAAB_8/7a1CYjp40nUg4lKpW7covGZJQAySxlg8QCLcBGAs/s640/transform20fps.gif)\n",
        "\n",
        "\n",
        "## BERT\n",
        "\n",
        "\n",
        "\n",
        "ELMo’s language model is bi-directional LSTMs.\n",
        "\n",
        "OpenAI GPT used  transformers but only trains a forward language model. \n",
        "\n",
        "BERT is  transformer-based model whose language model looks both forward and backwards.\n",
        "This bidirectional conditioning would allow each word to indirectly see itself in a multi-layered context, but BERT inplements a random token mask to overcome this. [more to come on why this works]\n",
        "\n",
        "BERT only used encoders from the transformer,  was trained using word pieces from BookCorpus (800M words) and english Wikipedia (2,500M words)and was trained with two objectives:\n",
        "\n",
        "*   \"Masked Language Model\": 15% of tokens from an input sequence are masked (some are randomly replaced with the wrong word). The model then needs to predict the id of this masked token. [description on how this impacts word's ability to see itself]\n",
        "```\n",
        "Input: the man went to the [MASK1] . he bought a [MASK2] of milk.\n",
        "Labels: [MASK1] = store; [MASK2] = gallon\n",
        "```\n",
        "\n",
        "*   Next Sentence Prediction: Takes input sentence pairs, replaces 50% of the second senteces with a random sentence, and trains to learn sentence relationships. Required for task like inferenceing and question answering. \n",
        "```\n",
        "Sentence A: the man went to the store .\n",
        "Sentence B: he bought a gallon of milk .\n",
        "Label: IsNextSentence\n",
        "```\n",
        "```\n",
        "Sentence A: the man went to the store .\n",
        "Sentence B: penguins are flightless .\n",
        "Label: NotNextSentence\n",
        "```\n",
        "**English BERT Models**\n",
        "\n",
        "```\n",
        "BERT-Base, Uncased: 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "BERT-Large, Uncased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "BERT-Base, Cased: 12-layer, 768-hidden, 12-heads , 110M parameters\n",
        "BERT-Large, Cased: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "```\n",
        "Layer = number of transformer blocks\n",
        "\n",
        "Hidden = hidden size\n",
        "\n",
        "Heads = number of self attention heads\n",
        "\n",
        "\n",
        "![alt text](https://1.bp.blogspot.com/-RLAbr6kPNUo/W9is5FwUXmI/AAAAAAAADeU/5y9466Zoyoc96vqLjbruLK8i_t8qEdHnQCLcBGAs/s1600/image3.png)\n",
        "\n",
        "What can task could you perform NLP transfer learning with BERT?\n",
        "\n",
        "![link text](https://cdn-images-1.medium.com/max/800/1*onB14-5wddG6Ij6BC3t2Xw.png)\n",
        "\n",
        "\n",
        "#### Noteworthy Details\n",
        "*  Model trained on WordPiece embeddings with ## denoting split word pieces.\n",
        " item\n",
        "*   List item\n",
        "\n",
        "\n",
        "\n",
        "#### BERT Pytorch Implementation\n",
        "[https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "\n",
        "This Notebook will walk you through all the relevant pytorch nn modules related to BERT and look at application in question answering. "
      ]
    },
    {
      "metadata": {
        "id": "JXLSdJbS3fa5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Bert Pytorch Implementation"
      ]
    },
    {
      "metadata": {
        "id": "luPs24JmC8OB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "1rb1AYLCfZCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NLk06HC-CdeM",
        "colab_type": "code",
        "outputId": "003b8d32-6c23-4fe5-bac8-5f5c39071f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/huggingface/pytorch-pretrained-BERT.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-pretrained-BERT'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Oz6gFKv2CspQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cp pytorch-pretrained-BERT/examples/run_squad.py ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Mznyl-Nh0rm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import of run_squad will fail due to issue calling args from file_utils with \n",
        "version of pytorch-pretrained-bert from pip this cell deletes that line\n",
        "\n",
        "deleted line:\n",
        "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, \n",
        "WEIGHTS_NAME, CONFIG_NAME\n",
        "\"\"\"\n",
        "with open(\"run_squad.py\", \"r\") as infile:\n",
        "    lines = infile.readlines()\n",
        "with open(\"run_squad.py\", \"w\") as outfile:\n",
        "    for pos, line in enumerate(lines):\n",
        "        if pos != 36:\n",
        "            outfile.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7OeDI4ho3em-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import tempfile\n",
        "import sys\n",
        "from io import open\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from pytorch_pretrained_bert import modeling\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from pytorch_pretrained_bert.modeling import load_tf_weights_in_bert, BertConfig\n",
        "\n",
        "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, cached_path \n",
        "from run_squad import *\n",
        "\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wqiGwDSpfsPs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mannually define config name and weights name args due to issue with version of pytorch-pretrained-bert from pip\n",
        "CONFIG_NAME = \"config.json\"\n",
        "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
        "PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n",
        "}\n",
        "BERT_CONFIG_NAME = 'bert_config.json'\n",
        "TF_WEIGHTS_NAME = 'model.ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W_W4eu7vKztz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Activation Functions in BERT\n",
        "\n",
        "### Gaussian Error Linear Units (GELU)\n",
        "Also used as an alternative to ReLU, GELU can be found two ways:\n",
        "####  **faster approximation -  sigmoid(1.702 * x) * x** \n",
        "####  **fslower, more accurate, actual - 0.5 * x * (1 + erf(x * 0.7978845608 * (1 + 0.044715 * x * x)))**\n",
        "erf, aka the error function or Gauss error function, is used in integrating a normal distribution. \n",
        "It is supposedly very close to tanh, ut I need to research this more.\n",
        "https://en.wikipedia.org/wiki/Error_function\n",
        "\n",
        "![link text](https://cdn-images-1.medium.com/max/800/1*XAJdlKP_tHY6dWuTnKc6lg.png)\n",
        "\n",
        "https://towardsdatascience.com/aifortrading-2edd6fac689d\n",
        "\n",
        "\n",
        "\n",
        "### Swish\n",
        "Developed by Google Brain. Similarity to ReLU makes it an easy replacment, and funtion tends to work better on deeper models.\n",
        "Replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\\%% for Mobile NASNet-A and 0.6\\%% for Inception-ResNet-v2. https://ai.google/research/pubs/pub46503\n",
        "\n",
        "####  **f(x) = x · sigmoid(x)**\n",
        "\n",
        "![link text](https://cdn-images-1.medium.com/max/800/1*6tUAKb3_1uGybwSufey3ZA.tiff)\n",
        "\n",
        "\"Swish sits somewhere between linear and ReLU activations. If we consider a simple variation of Swish f(x)=2x*sigmoid(beta*x) where beta is a learnable parameter, it can be seen that if beta = 0 the sigmoid part is always 1/2, so f(x) becomes linear. On the other hand if beta is very high, the sigmoid part becomes almost like a binary activation (0 for x<0 and 1 for x>0). Thus, f(x) approaches the ReLU function. Therefore, the standard swish function (beta=1) provides a smooth interpolation between these two extremes.\"\n",
        "https://medium.com/@jaiyamsharma/experiments-with-swish-activation-function-on-mnist-dataset-fc89a8c79ff7\n"
      ]
    },
    {
      "metadata": {
        "id": "TyuObNc8KTHs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IntMTNT0L3TN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# BERT Embedings:\n",
        "As you can see from the diagram below, this nn module is designed to supply the necissary embeddings for BERT's transformers.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/0*m_kXt3uqZH9e7H4w.png)\n",
        "\n",
        "\n",
        "#### Input Notes:\n",
        "A [CLS] token is added at the start of ever sequence.\n",
        "For training of task such as question answering, sentence pairs can be feed into BERT.  the first sentence and a [SEP] token at the end of each sentence. \n",
        "\n",
        "#### Token Embeddings:\n",
        "This layer converts each wordpiece token into a 768 dimenstional representation. \n",
        "\n",
        "#### Sentence Embeddings:\n",
        "If two senteces are fed into BERT, this layer assigned a 0 or 1 to each token in the input. This allows for the sequences to be differentiated later in the model. The output is also a 768 dimenstional representation. \n",
        "\n",
        "#### Transformer Positional Embeddings:\n",
        "As mentioned earlier in the transformer section, this architecture does not intake a sequence like RNNs. This embeding asigns a positional embedding to each token in the input. Since BERT is only designed to take a sequence up to 512 tokens in length, the layer is reall just a positional look up table. \n",
        "\n",
        "#### Combined Embeddings:\n",
        "All three embeddings are added together element wise. \n",
        "```\n",
        "embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "```\n",
        "\n",
        "#### Structure: \n",
        "```\n",
        "(embeddings): BertEmbeddings(\n",
        "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
        "      (position_embeddings): Embedding(512, 768)\n",
        "      (token_type_embeddings): Embedding(2, 768)\n",
        "      (LayerNorm): BertLayerNorm()\n",
        "      (dropout): Dropout(p=0.1)\n",
        "    )\n",
        "```\n",
        "\n",
        "Great Explanation:\n",
        "https://medium.com/@_init_/why-bert-has-3-embedding-layers-and-their-implementation-details-9c261108e28a\n"
      ]
    },
    {
      "metadata": {
        "id": "pejyynPdKTME",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A2WE2x55MBzm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building Towards the Encoder: BERT Attention, Self Attention, Intermediate and Layer\n",
        "\n",
        "#### Structure:\n",
        "```\n",
        "    (encoder): BertEncoder(\n",
        "      (layer): ModuleList(\n",
        "        (0): BertLayer(\n",
        "          (attention): BertAttention(\n",
        "            (self): BertSelfAttention(\n",
        "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (dropout): Dropout(p=0.1)\n",
        "            )\n",
        "            (output): BertSelfOutput(\n",
        "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (LayerNorm): BertLayerNorm()\n",
        "              (dropout): Dropout(p=0.1)\n",
        "            )\n",
        "          )\n",
        "          (intermediate): BertIntermediate(\n",
        "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "          )\n",
        "          (output): BertOutput(\n",
        "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "            (LayerNorm): BertLayerNorm()\n",
        "            (dropout): Dropout(p=0.1)\n",
        "          )\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "YkRXfOfDZC8D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The BERT Encoder\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/encoder_with_tensors_2.png)\n",
        "\n",
        "\n",
        "### $z$ \n",
        "```\n",
        "   (output): BertSelfOutput(\n",
        "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (LayerNorm): BertLayerNorm()\n",
        "              (dropout): Dropout(p=0.1)\n",
        "```\n",
        "\n",
        " ### Feed Forward\n",
        "```\n",
        "(intermediate): BertIntermediate(\n",
        "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
        "```\n",
        "activation fn is gelu by default\n",
        "\n",
        "hidden_act: The non-linear activation function (function or string) in the encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
        "\n",
        "\n",
        "\n",
        "### $r$\n",
        "```\n",
        "(output): BertOutput(\n",
        "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
        "            (LayerNorm): BertLayerNorm()\n",
        "            (dropout): Dropout(p=0.1)\n",
        "   ```"
      ]
    },
    {
      "metadata": {
        "id": "AQberEnDKTbh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        layer = BertLayer(config)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states, attention_mask)\n",
        "            if output_all_encoded_layers:\n",
        "                all_encoder_layers.append(hidden_states)\n",
        "        if not output_all_encoded_layers:\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        return all_encoder_layers\n",
        "      \n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "     \n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "      \n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "      \n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8dmfOEjtLuSb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BERT Self Attention and Attention\n",
        "\n",
        "\"An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
        "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
        "of the values, where the weight assigned to each value is computed by a compatibility function of the\n",
        "query with the corresponding key\"\n",
        "https://arxiv.org/pdf/1706.03762.pdf\n",
        "\n",
        "![link text](http://jalammar.github.io/images/t/transformer_self_attention_vectors.png)\n",
        "\n",
        "\"What are the “query”, “key”, and “value” vectors? \n",
        "\n",
        "They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.\n",
        "\n",
        "The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
        "\n",
        "The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\" http://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "\n",
        "**Vector representation of Self Attention process, actual proccess is done using matixies **\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/self-attention-output.png)\n",
        "\n",
        "\n",
        "The query is usually the hidden state of the decoder. \n",
        "Key is the hidden state of the encoder and the corresponding value is normalized weight, representing how much attention a key gets. \n",
        "Output is calculated as a wighted sum.\n",
        "\n",
        "\n",
        "\n",
        "divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. \n",
        "\n",
        "multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example)\n",
        "\n",
        "sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word)\n",
        "```\n",
        "(output): BertSelfOutput(\n",
        "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
        "              (LayerNorm): BertLayerNorm()\n",
        "              (dropout): Dropout(p=0.1)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Yf-wVvDLKTOt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PrgLPvJqiupo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Multi Headed Attention\n",
        "Now in reality, BERT calculates attention over multiple heads in a layer of the stack.\n",
        "\n",
        "\"This improves the performance of the attention layer in two ways:\n",
        "\n",
        "It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the the actual word itself. It would be useful if we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, we would want to know which word “it” refers to.\n",
        "\n",
        "It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\"\n",
        "\n",
        "\n",
        "![alt text](http://jalammar.github.io/images/t/transformer_attention_heads_qkv.png)"
      ]
    },
    {
      "metadata": {
        "id": "lpzO7refNgFw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### BERT Intermediate: $Feed Forward Net$\n"
      ]
    },
    {
      "metadata": {
        "id": "YYB5mKXQKTSB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4bnmUVXgNm9x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "explain output"
      ]
    },
    {
      "metadata": {
        "id": "8rn77ysUKTVL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uTI7NwkQPPOZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Explain the pretrained model\n",
        "\n",
        "review pretained model options"
      ]
    },
    {
      "metadata": {
        "id": "7Mk0XZR9PQcq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertPreTrainedModel(nn.Module):\n",
        "    \"\"\" An abstract class to handle weights initialization and\n",
        "        a simple interface for dowloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(BertPreTrainedModel, self).__init__()\n",
        "        if not isinstance(config, BertConfig):\n",
        "            raise ValueError(\n",
        "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
        "                \"To create a model from a Google pretrained model use \"\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
        "                    self.__class__.__name__, self.__class__.__name__\n",
        "                ))\n",
        "        self.config = config\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None,\n",
        "                        from_tf=False, *inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n",
        "        Download and cache the pre-trained model file if needed.\n",
        "        Params:\n",
        "            pretrained_model_name_or_path: either:\n",
        "                - a str with the name of a pre-trained model to load selected in the list of:\n",
        "                    . `bert-base-uncased`\n",
        "                    . `bert-large-uncased`\n",
        "                    . `bert-base-cased`\n",
        "                    . `bert-large-cased`\n",
        "                    . `bert-base-multilingual-uncased`\n",
        "                    . `bert-base-multilingual-cased`\n",
        "                    . `bert-base-chinese`\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `model.chkpt` a TensorFlow checkpoint\n",
        "            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n",
        "            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n",
        "            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n",
        "            *inputs, **kwargs: additional input for the specific Bert class\n",
        "                (ex: num_labels for BertForSequenceClassification)\n",
        "        \"\"\"\n",
        "        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
        "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n",
        "        else:\n",
        "            archive_file = pretrained_model_name_or_path\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n",
        "        except EnvironmentError:\n",
        "            logger.error(\n",
        "                \"Model name '{}' was not found in model name list ({}). \"\n",
        "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                \"associated to this path or url.\".format(\n",
        "                    pretrained_model_name_or_path,\n",
        "                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
        "                    archive_file))\n",
        "            return None\n",
        "        if resolved_archive_file == archive_file:\n",
        "            logger.info(\"loading archive file {}\".format(archive_file))\n",
        "        else:\n",
        "            logger.info(\"loading archive file {} from cache at {}\".format(\n",
        "                archive_file, resolved_archive_file))\n",
        "        tempdir = None\n",
        "        if os.path.isdir(resolved_archive_file) or from_tf:\n",
        "            serialization_dir = resolved_archive_file\n",
        "        else:\n",
        "            # Extract archive to temp dir\n",
        "            tempdir = tempfile.mkdtemp()\n",
        "            logger.info(\"extracting archive file {} to temp dir {}\".format(\n",
        "                resolved_archive_file, tempdir))\n",
        "            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
        "                archive.extractall(tempdir)\n",
        "            serialization_dir = tempdir\n",
        "        # Load config\n",
        "        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
        "        if not os.path.exists(config_file):\n",
        "            # Backward compatibility with old naming format\n",
        "            config_file = os.path.join(serialization_dir, BERT_CONFIG_NAME)\n",
        "        config = BertConfig.from_json_file(config_file)\n",
        "        logger.info(\"Model config {}\".format(config))\n",
        "        # Instantiate model.\n",
        "        model = cls(config, *inputs, **kwargs)\n",
        "        if state_dict is None and not from_tf:\n",
        "            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
        "            state_dict = torch.load(weights_path, map_location='cpu')\n",
        "        if tempdir:\n",
        "            # Clean up temp dir\n",
        "            shutil.rmtree(tempdir)\n",
        "        if from_tf:\n",
        "            # Directly load from a TensorFlow checkpoint\n",
        "            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n",
        "            return load_tf_weights_in_bert(model, weights_path)\n",
        "        # Load from a PyTorch state_dict\n",
        "        old_keys = []\n",
        "        new_keys = []\n",
        "        for key in state_dict.keys():\n",
        "            new_key = None\n",
        "            if 'gamma' in key:\n",
        "                new_key = key.replace('gamma', 'weight')\n",
        "            if 'beta' in key:\n",
        "                new_key = key.replace('beta', 'bias')\n",
        "            if new_key:\n",
        "                old_keys.append(key)\n",
        "                new_keys.append(new_key)\n",
        "        for old_key, new_key in zip(old_keys, new_keys):\n",
        "            state_dict[new_key] = state_dict.pop(old_key)\n",
        "\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "        error_msgs = []\n",
        "        # copy state_dict so _load_from_state_dict can modify it\n",
        "        metadata = getattr(state_dict, '_metadata', None)\n",
        "        state_dict = state_dict.copy()\n",
        "        if metadata is not None:\n",
        "            state_dict._metadata = metadata\n",
        "\n",
        "        def load(module, prefix=''):\n",
        "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
        "            module._load_from_state_dict(\n",
        "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
        "            for name, child in module._modules.items():\n",
        "                if child is not None:\n",
        "                    load(child, prefix + name + '.')\n",
        "        start_prefix = ''\n",
        "        if not hasattr(model, 'bert') and any(s.startswith('bert.') for s in state_dict.keys()):\n",
        "            start_prefix = 'bert.'\n",
        "        load(model, prefix=start_prefix)\n",
        "        if len(missing_keys) > 0:\n",
        "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
        "                model.__class__.__name__, missing_keys))\n",
        "        if len(unexpected_keys) > 0:\n",
        "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
        "                model.__class__.__name__, unexpected_keys))\n",
        "        if len(error_msgs) > 0:\n",
        "            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
        "                               model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_5UYOYNLODjm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Explain BERT's encoder"
      ]
    },
    {
      "metadata": {
        "id": "9Hg0wlffOii0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "fine tuning with the pooler explanation "
      ]
    },
    {
      "metadata": {
        "id": "gWhV3LmvKTeH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j9ZqZLVEO9pB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With all these peices combined we have the bert model!\n",
        "Explain BERT"
      ]
    },
    {
      "metadata": {
        "id": "bTP7SPgNKThv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertModel(BertPreTrainedModel):\n",
        "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "    Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "            classifier pretrained on top of the hidden state associated to the first character of the\n",
        "            input (`CLS`) to train on the Next-Sentence task (see BERT's paper).\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    model = modeling.BertModel(config=config)\n",
        "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dhqTVtn-rtIN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "recap on BERT"
      ]
    },
    {
      "metadata": {
        "id": "9bg0PJGNRhp7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementing BERT\n",
        "Now that you understand BERT,  let's celebrate by implementing BERT on the SQuAD 2.0 challenge!\n",
        "\n",
        "![alt text](https://media.giphy.com/media/umMYB9u0rpJyE/giphy.gif)\n",
        "\n",
        "## What's a SQuAD 2.0?\n",
        "explain challenge \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D-txgryFQBov",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Bert for Q&A explanations \n",
        "outputs start and end positions "
      ]
    },
    {
      "metadata": {
        "id": "gZF8iQWVKTka",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BertForQuestionAnswering(BertPreTrainedModel):\n",
        "    \"\"\"BERT model for Question Answering (span extraction).\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the sequence output that computes start_logits and end_logits\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n",
        "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
        "            into account for computing the loss.\n",
        "        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n",
        "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
        "            into account for computing the loss.\n",
        "    Outputs:\n",
        "        if `start_positions` and `end_positions` are not `None`:\n",
        "            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n",
        "        if `start_positions` or `end_positions` is `None`:\n",
        "            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n",
        "            position tokens of shape [batch_size, sequence_length].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    model = BertForQuestionAnswering(config)\n",
        "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n",
        "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n",
        "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "            return total_loss\n",
        "        else:\n",
        "            return start_logits, end_logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_FS1v4n45QiB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Prepare optimizer\n",
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "# hack to remove pooler, which is not used\n",
        "# thus it produce None grad that break apex\n",
        "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,lr=learning_rate,\n",
        "                warmup=warmup_proportion,\n",
        "                t_total=num_train_optimization_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ccLciscBFId2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "let's load up the data and make some features.... takes a while. go grab some coffee."
      ]
    }
  ]
}